import os
import torch
import gradio as gr
import shutil
import re
from TTS.api import TTS
from huggingface_hub import hf_hub_download
from pydub import AudioSegment
from brain import MahagyaniBrain # ‡§Ü‡§™‡§ï‡§æ ‡§ó‡§ø‡§ü‡§π‡§¨ ‡§µ‡§æ‡§≤‡§æ ‡§¨‡•ç‡§∞‡•á‡§®

# ‚ö° ‡§ü‡§∞‡•ç‡§¨‡•ã ‡§π‡§æ‡§à ‡§∏‡•ç‡§™‡•Ä‡§° ‡§∏‡•á‡§ü‡§Ö‡§™ [cite: 2026-01-06]
os.environ["COQUI_TOS_AGREED"] = "1"
device = "cuda" if torch.cuda.is_available() else "cpu"

# üì• ‡§Æ‡§æ‡§∏‡•ç‡§ü‡§∞ ‡§Æ‡•â‡§°‡§≤ [cite: 2026-02-16]
REPO_ID = "Shriramnag/My-Shriram-Voice" 
MODEL_FILE = "Ramai.pth"
model_path = hf_hub_download(repo_id=REPO_ID, filename=MODEL_FILE)
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)

# üß† ‡§Æ‡§π‡§æ‡§ú‡•ç‡§û‡§æ‡§®‡•Ä ‡§¨‡•ç‡§∞‡•á‡§® ‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§® (LOCKED)
brain = MahagyaniBrain(
    'sanskrit_knowledge.json', 
    'hindi_grammar.json', 
    'english_knowledge.json', 
    'prosody_config.json'
)

def apply_final_mastering(file_path, amp, pitch_val):
    """‡§á‡§ï‡•ã ‡§∏‡•Å‡§ß‡§æ‡§∞ (-42dB) ‡§î‡§∞ ‡§ï‡•ç‡§∞‡§ø‡§∏‡•ç‡§ü‡§≤ ‡§ï‡•ç‡§≤‡•à‡§∞‡§ø‡§ü‡•Ä [cite: 2026-01-06]"""
    sound = AudioSegment.from_wav(file_path)
    sound = sound + amp 
    new_rate = int(sound.frame_rate * pitch_val)
    sound = sound._spawn(sound.raw_data, overrides={'frame_rate': new_rate}).set_frame_rate(44100)
    
    # ‚úÖ ‡§á‡§ï‡•ã ‡§ï‡§Æ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§§‡§æ‡§ï‡§ø ‡§π‡§ï‡§≤‡§æ‡§π‡§ü ‡§® ‡§π‡•ã
    echo = sound - 42 
    sound = sound.overlay(echo, position=180) 
    
    sound = sound.low_pass_filter(4000)
    return sound

def generate_voice(text, voice_sample, speed_s, pitch_s, weight_s, amp_s, progress=gr.Progress()):
    # 1. ‡§¨‡•ç‡§∞‡•á‡§® ‡§∏‡•á ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§∂‡•Å‡§¶‡•ç‡§ß ‡§ï‡§∞‡§®‡§æ (‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§/‡§π‡§ø‡§Ç‡§¶‡•Ä/‡§á‡§Ç‡§ó‡•ç‡§≤‡§ø‡§∂) [cite: 2026-02-18]
    cleaned_text = brain.clean_and_format(text)
    profile = brain.get_voice_profile(text)
    
    # ‡§Ö‡§ó‡§∞ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§ ‡§∂‡•ç‡§≤‡•ã‡§ï ‡§π‡•à ‡§§‡•ã ‡§¨‡•ç‡§∞‡•á‡§® ‡§ï‡•Ä ‡§´‡§ø‡§ï‡•ç‡§∏‡•ç‡§° ‡§∏‡•ç‡§™‡•Ä‡§° ‡§≤‡•á‡§Ç, ‡§µ‡§∞‡§®‡§æ ‡§∏‡•ç‡§≤‡§æ‡§á‡§°‡§∞ ‡§ï‡•Ä
    final_speed = profile['global_speed'] if "‡••" in text else speed_s
    
    # 2. ‡§ö‡§Ç‡§ï‡§ø‡§Ç‡§ó ‡§≤‡•â‡§ú‡§ø‡§ï (LOCKED) [cite: 2026-02-16]
    sentences = re.split('([‡•§!?‡••])', cleaned_text)
    chunks = []
    for i in range(0, len(sentences)-1, 2):
        chunks.append(sentences[i] + sentences[i+1])
    
    chunk_files = []
    output_folder = "temp_chunks"
    if os.path.exists(output_folder): shutil.rmtree(output_folder)
    os.makedirs(output_folder)

    for i, chunk in enumerate(chunks):
        progress((i+1)/len(chunks), desc="üöÄ ‡§ü‡§∞‡•ç‡§¨‡•ã ‡§Æ‡§π‡§æ‡§ú‡•ç‡§û‡§æ‡§®‡•Ä ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó...")
        name = os.path.join(output_folder, f"c_{i}.wav")
        tts.tts_to_file(
            text=chunk, speaker_wav=voice_sample, language="hi", file_path=name,
            speed=final_speed, repetition_penalty=15.0, # ‡§®‡•ã ‡§π‡§ï‡§≤‡§æ‡§π‡§ü ‡§´‡§ø‡§ï‡•ç‡§∏
            temperature=0.75, top_p=0.85
        )
        chunk_files.append(name)

    combined = AudioSegment.empty()
    for f in chunk_files: combined += AudioSegment.from_wav(f)
    
    # 3. ‡§´‡§æ‡§á‡§®‡§≤ ‡§Æ‡§æ‡§∏‡•ç‡§ü‡§∞‡§ø‡§Ç‡§ó [cite: 2026-01-06]
    combined.export("temp.wav", format="wav")
    final_audio = apply_final_mastering("temp.wav", amp_s, pitch_s)
    final_audio.export("shriram_final.wav", format="wav")
    return "shriram_final.wav"

# üé® UI - ‡§∏‡§≠‡•Ä ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§´‡•Ä‡§ö‡§∞‡•ç‡§∏ ‡§î‡§∞ ‡§∏‡•ç‡§≤‡§æ‡§á‡§°‡§∞‡•ç‡§∏ ‡§µ‡§æ‡§™‡§∏ ‡§Ü ‡§ó‡§è ‡§π‡•à‡§Ç
with gr.Blocks(theme=gr.themes.Soft(primary_hue="orange")) as demo:
    gr.Markdown("# üö© ‡§∂‡•ç‡§∞‡•Ä‡§∞‡§æ‡§Æ ‡§µ‡§æ‡§£‡•Ä - ‡§Æ‡§π‡§æ‡§ú‡•ç‡§û‡§æ‡§®‡•Ä ‡§µ‡§∞‡•ç‡§ú‡§® (‡§∏‡§¨ ‡§ï‡•Å‡§õ ‡§´‡§ø‡§ï‡•ç‡§∏‡•ç‡§°)")
    with gr.Row():
        with gr.Column(scale=2):
            txt = gr.Textbox(label="‡§Ø‡§π‡§æ‡§Å ‡§∂‡•ç‡§≤‡•ã‡§ï ‡§Ø‡§æ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§≤‡§ø‡§ñ‡•á‡§Ç", lines=12)
        with gr.Column(scale=1):
            ref = gr.Audio(label="‡§Æ‡§æ‡§∏‡•ç‡§ü‡§∞ ‡§∏‡•à‡§Ç‡§™‡§≤ (aideva.wav)", type="filepath")
            with gr.Accordion("‚öôÔ∏è ‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó‡•ç‡§∏ (LOCKED CONTROLS)", open=True):
                speed_s = gr.Slider(label="‡§∞‡•û‡•ç‡§§‡§æ‡§∞", minimum=0.8, maximum=1.2, value=1.0)
                pitch_s = gr.Slider(label="‡§™‡§ø‡§ö", minimum=0.8, maximum=1.1, value=0.96)
                weight_s = gr.Slider(label="‡§≠‡§æ‡§∞‡•Ä‡§™‡§®", minimum=0, maximum=10, value=6)
                amp_s = gr.Slider(label="‡§™‡§æ‡§µ‡§∞", minimum=-5, maximum=10, value=4)
            
            btn = gr.Button("‡§¶‡§ø‡§µ‡•ç‡§Ø ‡§Ü‡§µ‡§æ‡•õ ‡§ú‡§®‡§∞‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç üöÄ", variant="primary")
            
    out = gr.Audio(label="‡§∂‡•Å‡§¶‡•ç‡§ß ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü", type="filepath", autoplay=True)
    # üîÑ ‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§® ‡§ö‡•á‡§ï: [Text, Audio, Speed, Pitch, Weight, Amp]
    btn.click(generate_voice, [txt, ref, speed_s, pitch_s, weight_s, amp_s], out)

demo.launch(share=True)
