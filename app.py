import os
import torch
import gradio as gr
import shutil
import random
import re
from TTS.api import TTS
from huggingface_hub import hf_hub_download
from pydub import AudioSegment

# ‚ö° ‡§ü‡§∞‡•ç‡§¨‡•ã ‡§π‡§æ‡§à ‡§∏‡•ç‡§™‡•Ä‡§° ‡§∏‡•á‡§ü‡§Ö‡§™ [cite: 2026-01-06]
os.environ["COQUI_TOS_AGREED"] = "1"
device = "cuda" if torch.cuda.is_available() else "cpu"

# üì• ‡§Ü‡§™‡§ï‡§æ 'Ramai.pth' ‡§Æ‡•â‡§°‡§≤ [cite: 2026-02-16]
REPO_ID = "Shriramnag/My-Shriram-Voice" 
MODEL_FILE = "Ramai.pth"
model_path = hf_hub_download(repo_id=REPO_ID, filename=MODEL_FILE)
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)

def clean_hindi_text(text):
    """‡§®‡§Ç‡§¨‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§®‡§æ (Error Fix)"""
    num_map = {'0':'‡§∂‡•Ç‡§®‡•ç‡§Ø','1':'‡§è‡§ï','2':'‡§¶‡•ã','3':'‡§§‡•Ä‡§®','4':'‡§ö‡§æ‡§∞','5':'‡§™‡§æ‡§Ç‡§ö','6':'‡§õ‡§π','7':'‡§∏‡§æ‡§§','8':'‡§Ü‡§†','9':'‡§®‡•å'}
    for k, v in num_map.items():
        text = text.replace(k, v)
    return text

def split_into_chunks(text):
    """‡§™‡•Å‡§∞‡§æ‡§®‡§æ ‡§µ‡§∞‡•ç‡§ï‡§ø‡§Ç‡§ó ‡§ö‡§Ç‡§ï‡§ø‡§Ç‡§ó ‡§≤‡•â‡§ú‡§ø‡§ï [cite: 2026-02-16]"""
    sentences = re.split('([‡•§!?])', text)
    chunks = []
    current_chunk = ""
    for i in range(0, len(sentences)-1, 2):
        sentence = sentences[i] + sentences[i+1]
        if len(current_chunk) + len(sentence) < 250:
            current_chunk += sentence
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
    if current_chunk: chunks.append(current_chunk.strip())
    return chunks

def apply_ultimate_100_match(file_path, weight, amp, pitch_val):
    """‡§Ü‡§µ‡§æ‡•õ ‡§ï‡•ã aideva.wav ‡§∏‡•á 100% ‡§Æ‡§ø‡§≤‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§∏‡•ç‡§ü‡§∞‡§ø‡§Ç‡§ó ‡§≤‡•â‡§ú‡§ø‡§ï"""
    sound = AudioSegment.from_wav(file_path)
    
    # üíé ‡§™‡§æ‡§µ‡§∞ ‡§î‡§∞ ‡§™‡§ø‡§ö ‡§¨‡•à‡§≤‡•á‡§Ç‡§∏
    sound = sound + amp 
    new_rate = int(sound.frame_rate * pitch_val)
    sound = sound._spawn(sound.raw_data, overrides={'frame_rate': new_rate}).set_frame_rate(44100)
    
    # ‚úÖ 100% ‡§Æ‡•à‡§ö ‡§´‡§ø‡§ï‡•ç‡§∏: ‡§°‡§¨‡§≤ ‡§≤‡•á‡§Ø‡§∞ ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§Æ ‡§á‡§ï‡•ã (‡§∏‡§ø‡§®‡•á‡§Æ‡•à‡§ü‡§ø‡§ï ‡§ó‡§π‡§∞‡§æ‡§à ‡§ï‡•á ‡§≤‡§ø‡§è)
    # ‡§™‡§π‡§≤‡•Ä ‡§≤‡•á‡§Ø‡§∞: ‡§¨‡§π‡•Å‡§§ ‡§π‡§≤‡•ç‡§ï‡•Ä ‡§î‡§∞ ‡§§‡•á‡•õ (‡§Ü‡§µ‡§æ‡•õ ‡§ï‡•ã ‡§≠‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è)
    echo_1 = sound - 28
    sound = sound.overlay(echo_1, position=160)
    
    # ‡§¶‡•Ç‡§∏‡§∞‡•Ä ‡§≤‡•á‡§Ø‡§∞: ‡§•‡•ã‡•ú‡•Ä ‡§ó‡§π‡§∞‡•Ä (‡§∏‡§ø‡§®‡•á‡§Æ‡•à‡§ü‡§ø‡§ï ‡§Ö‡§π‡§∏‡§æ‡§∏ ‡§ï‡•á ‡§≤‡§ø‡§è)
    echo_2 = sound - 32
    sound = sound.overlay(echo_2, position=280)
    
    # üîä ‡§¨‡•á‡§∏ ‡§¨‡•Ç‡§∏‡•ç‡§ü‡§∞: aideva.wav ‡§ú‡•à‡§∏‡•Ä ‡§ó‡§Ç‡§≠‡•Ä‡§∞‡§§‡§æ ‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è
    sound = sound.low_pass_filter(3800)
    
    final_path = "shriram_final_100_percent.wav"
    sound.export(final_path, format="wav")
    return final_path

def generate_voice(text, voice_sample, speed, human_feel, weight, amp, pitch_val, progress=gr.Progress()):
    text = clean_hindi_text(text)
    chunks = split_into_chunks(text)
    chunk_files = []
    output_folder = "temp_chunks"
    if os.path.exists(output_folder): shutil.rmtree(output_folder)
    os.makedirs(output_folder)

    for i, chunk in enumerate(chunks):
        progress((i+1)/len(chunks), desc=f"üöÄ 100% ‡§Æ‡•à‡§ö‡§ø‡§Ç‡§ó ‡§ú‡§æ‡§∞‡•Ä: {i+1}/{len(chunks)}")
        name = os.path.join(output_folder, f"c_{i}.wav")
        
        # üé≠ ‡§Ö‡§∏‡§≤‡•Ä ‡§á‡§Ç‡§∏‡§æ‡§®‡•Ä ‡§Ö‡§π‡§∏‡§æ‡§∏ (Jitter)
        dynamic_temp = human_feel + random.uniform(-0.04, 0.04)
        
        tts.tts_to_file(
            text=chunk, speaker_wav=voice_sample, language="hi", file_path=name,
            speed=speed, repetition_penalty=11.0, temperature=dynamic_temp,
            top_p=0.82, gpt_cond_len=9 # ‡§ï‡§Ç‡§°‡§ø‡§∂‡§®‡§ø‡§Ç‡§ó ‡§¨‡•ù‡§æ‡§à ‡§§‡§æ‡§ï‡§ø ‡§Æ‡§æ‡§∏‡•ç‡§ü‡§∞ ‡§∏‡•à‡§Ç‡§™‡§≤ ‡§ï‡•Ä ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§™‡§ï‡•ú‡•á
        )
        chunk_files.append(name)

    combined = AudioSegment.empty()
    for f in chunk_files: combined += AudioSegment.from_wav(f)
    combined.export("combined.wav", format="wav")
    
    return apply_ultimate_100_match("combined.wav", weight, amp, pitch_val)

# üé® 100% ‡§Æ‡•à‡§ö '‡§∞‡•â‡§Ø‡§≤' UI
with gr.Blocks(theme=gr.themes.Soft(primary_hue="orange")) as demo:
    gr.Markdown("# üö© ‡§∂‡•ç‡§∞‡•Ä‡§∞‡§æ‡§Æ ‡§µ‡§æ‡§£‡•Ä - 100% ‡§Æ‡•à‡§ö ‡§Æ‡§æ‡§∏‡•ç‡§ü‡§∞ (Final)")
    
    with gr.Row():
        with gr.Column(scale=2):
            txt = gr.Textbox(label="‡§Ö‡§™‡§®‡•Ä ‡§Ö‡§Æ‡•É‡§§ ‡§µ‡§æ‡§£‡•Ä ‡§Ø‡§π‡§æ‡§Å ‡§≤‡§ø‡§ñ‡•á‡§Ç", lines=12)
        with gr.Column(scale=1):
            ref = gr.Audio(label="‡§Æ‡§æ‡§∏‡•ç‡§ü‡§∞ ‡§∏‡•à‡§Ç‡§™‡§≤ (aideva.wav)", type="filepath")
            with gr.Accordion("‚öôÔ∏è ‡§Æ‡§æ‡§∏‡•ç‡§ü‡§∞ ‡§ï‡§Ç‡§ü‡•ç‡§∞‡•ã‡§≤ (100% ‡§™‡§∞‡§´‡•á‡§ï‡•ç‡§ü)", open=True):
                speed_s = gr.Slider(label="‡§∞‡•û‡•ç‡§§‡§æ‡§∞", minimum=0.8, maximum=1.2, value=1.0)
                pitch_s = gr.Slider(label="‡§™‡§ø‡§ö (‡§ó‡§Ç‡§≠‡•Ä‡§∞‡§§‡§æ)", minimum=0.8, maximum=1.1, value=0.95)
                human_s = gr.Slider(label="‡§á‡§Æ‡•ã‡§∂‡§® ‡§ü‡§ö", minimum=0.5, maximum=1.0, value=0.92)
                weight_s = gr.Slider(label="‡§≠‡§æ‡§∞‡•Ä‡§™‡§® (Bass)", minimum=0, maximum=10, value=8)
                amp_s = gr.Slider(label="‡§™‡§æ‡§µ‡§∞ (Gain)", minimum=-5, maximum=10, value=5)
            
            btn = gr.Button("‡§Ü‡§µ‡§æ‡•õ ‡§ú‡§®‡§∞‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç üöÄ", variant="primary")
            
    out = gr.Audio(label="100% ‡§Æ‡•à‡§ö ‡§∂‡•ç‡§∞‡•Ä‡§∞‡§æ‡§Æ ‡§µ‡§æ‡§£‡•Ä", type="filepath", autoplay=True)
    btn.click(generate_voice, [txt, ref, speed_s, human_s, weight_s, amp_s, pitch_s], out)

demo.launch(share=True)
